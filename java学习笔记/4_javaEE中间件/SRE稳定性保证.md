### SRE稳定性保障

SRE 是一套体系化的工程。从职能分工上，SRE 体系建设不是单个岗位或单个部门就能独立完成的，需要高效的跨团队组织协作。

**SRE稳定性指标：**

* MTBF，Mean Time Between Failure，平均故障时间间隔。 提升 MTBF，也减少故障发生次数，提升故障发生间隔时长。
* MTTR，Mean Time To Repair， 故障平均修复时间。降低 MTTR，提升故障处理效率，减少故障影响时长。

**SRE根本目标：**减少故障时间，增加系统正常运行时间，也就是 “减少故障，提升 MTBF；同时提升故障处理效率，降低 MTTR”。

![image-20210608192323213](/Users/zhangxiaoxia/git_dir/learn/images/SRE稳定性保障.png)



**MTTR 细分为 4 个指标：**

* MTTI (Mean time to Identify，平均故障发现时间)：标识故障发生到开始真正响应的时间。可能通过用户或者客服反馈、舆情感知，监控告警渠道触发。
* MTTK (Mean time to Know，平均故障认知时间)：平均故障定位时间，也就是根因定位时间。
* MTTF (Mean time to Fix，平均故障解决时间)：采取措施恢复业务的时间。常见有限流，降级，熔断，回滚和重启。
* MTTV (Mean time to Verify，平均故障修复验证时间)：故障解决后，通过用户反馈或者监控手段来确认真正业务恢复所用的时间。





#### 稳定性指标

##### 系统可用性

衡量系统可用性：

* 时间维度：Availability = Uptime / (Uptime + Downtime)
* 请求维度：Availability = Successful request / Total request



**时间维度：**可用时长 和 成功请求 进行衡量和判定可用性

* 衡量指标：比如衡量指标，TPS，QPS，RT，状态码
* 衡量目标：达到什么目标是正常，达不到就是异常。
* 影响时长

例如：请求状态码为非 5xx 的比例【指标】，请求成功率低于 95%【目标】，已经连续超过 10 分钟【响应时长】，就要算作故障。

基于时间维度的可用性 会和真实情况有偏差：一个系统因为网络抖动，有短暂的几秒、十几秒，或者几分钟异常，但是后来系统自己恢复了，业务并没有中断，我们按照时长维度来判断，肯定不会算作系统故障。但是如果这种短暂的影响频度非常高，一天来个 5、6 次，持续一两周，我们应该可以判定系统运行状况也是不正常的，可能不是故障，但系统肯定是不稳定的。所以这种用时长维度来衡量系统稳定性的方式，其主要缺点就是粒度不够精细。



**请求维度：**进行衡量和判定可用性

* 衡量指标，请求成功率；
* 衡量目标，达到什么目标是正常，达不到就是异常。成功率达到 95% 才算系统运行正常；
* 统计周期，比如一天、一周、一个月等等，我们是在一个统计周期内计算整体状况，而不是看单次的。

例如：请求状态码为非 5xx 的比例【指标】，1个月内【统计周期】，请求成功率低于 95%【目标】，则系统稳定性不达标。具体可用性和允许故障时间对照表如下：

| 系统可用性 | 故障时间/年 | 故障时间/月 | 故障时间/周 | 故障时间/天 |
| :--------: | :---------: | :---------: | :---------: | :---------: |
|    90%     |   36.5天    |   72小时    |  16.8小时   |   2.4小时   |
|    99%     |   3.65天    |   7.2小时   |  1.68小时   |  14.4分钟   |
|   99.9%    |  8.76小时   |  43.8分钟   |  10.1分钟   |  1.44分钟   |
|   99.99%   |  52.56分钟  |  4.38分钟   |  1.01分钟   |   8.66秒    |
|  99.999%   |  5.26分钟   |   25.9秒    |   6.05秒    |   0.87秒    |





##### 选择SLI

**SLI**（Service Level Indicator）：服务等级指标，其实就是我们选择哪些指标来衡量我们的稳定性。

**可选的稳定性指标：**

* 系统层面：cpu使用率、load值、内存使用率、磁盘使用率、磁盘io、网络io

* 应用服务器层面：端口存活、JVM的GC情况等

* 应用运行层面：请求返回状态码、时延、应用层QPS、TPS以及连接数等

* 业务层面：在线用户数、新注册用户数、下单数、支付数、以及业务层成功率等指标



**SLI 指标的两大原则：**

* 原则一：选择能够标识一个主体是否稳定的指标，如果不是这个主体本身的指标，或者不能标识主体稳定性的，就要排除在外。
* 原则二：针对电商这类有用户界面的业务系统，优先选择与用户体验强相关或用户可以明显感知的指标。



**Google 选取SLI方法【VALET】** ==> 快速识别 SLI 指标的方法

* Volume（容量）：指服务承诺的最大容量是多少。比如，一个应用集群的 QPS、TPS、会话数以及连接数等等
* Availablity（可用性）代表服务是否正常。请求调用的非 5xx 状态码成功率，就可以归于可用性。
* Latency（时延）是说响应是否足够快。这是一个会直接影响用户访问体验的指标。例如 95% 请求的时延 <=120ms ”
* Errors- 错误率
* Tickets- 人工介入





##### 设定SLO

**SLO**（Service Level Objective）：服务等级目标，指的就是我们设定的稳定性目标，比如“几个 9”这样的目标。



第一种，直接根据成功的定义来计算。公式：Successful = （状态码非 5xx） & （时延 <= 80ms）

也就是我们前面定义一个请求的返回状态码必须是非 5xx 才算成功，同时时延还要低于 80ms，同时满足这两个条件，我们才算是成功的。通常云服务承诺的可用性就是采用这种方式。这种计算方式会有问题：网络抖动和数据抖动肯定会导致时延条件不满足，那么整个请求判定为不可用。假如数据库中坏请求比例为1 / 1000，那么当请求数量足够多的时候，我们的可用性肯定就不达标啦。



第二种方式：公式：Availability = SLO1 & SLO2 & SLO3

SLO 就是对应 SLI 要实现的目标：Availability = Successful request / Total request

SLO 方式计算：

* SLO1：99.95% 状态码成功率
* SLO2：90% Latency <= 80ms
* SLO3：99% Latency <= 200ms



**衡量 SLO 的有效性：**SLO不是拍脑袋决定的。

* SLO【达成情况】 ==> 达成（Met）或未达成（Missed）。
* Toil【“人肉”投入程度】==> 指需要大量人工投入、重复、繁琐且没有太多价值的事情。用投入程度高（High）和低（Low）来表示。
* Customer Satisfaction【用户满意度】 ==> 用户感受和体验如何.这个信息可以通过真实和虚拟渠道获得。真实渠道如客服投诉、客户访谈和舆情监控获取；虚拟渠道如真机模拟拨测。我们用满意度高（High）和低（Low）来表示。



|  SLOs  | Toil | Customer Satisfaction |  Action  |
| :----: | :--: | :-------------------: | :------: |
|  Met   | Low  |         High          | 保持现状 |
|  Met   | Low  |          Low          | 收紧 SLO |
|  Met   | High |         High          |          |
|  Met   | High |          Low          |          |
| Missed | Low  |         High          | 放宽 SLO |
| Missed | Low  |          Low          |          |
| Missed | High |         High          |          |
| Missed | High |          Low          |          |

第一类，收紧 SLO  ==> 比如 SLO 达成（Met），但是用户不满意（Low）。会有什么后果呢？要么投诉多，要么到处吐槽。这就表示我们的 SLO 设定得太容易达成，没有反馈真实的运行状况。

第二类，放宽 SLO  ==> 目标定太高，总是达不成（Missed），但用户反馈却很不错（High），这种就会造成错误预算提前消耗完，导致很多变更暂停，产品延期，甚至会做一些无谓的优化，这时就可以适当松松绑。

第三类，保持现状，对有问题的维度采取有针对性的优化措施。



##### 错误预算

Error Budget【错误预算】：最大的作用就是“提示你还有多少次犯错的机会”。

通过SLO反向推到错误预算：假设请求数1000000

|         SLO          |   Error Budget   |
| :------------------: | :--------------: |
| 99.95% Availability  | 1000000 * 0.0005 |
| 90% Latency <= 80ms  |  1000000 * 0.1   |
| 99% Latency <= 200ms | 10000000 * 0.01  |

**稳定性共识机制**：

* 第一，剩余预算充足或未消耗完之前，对问题的发生要有容忍度。
* 第二，剩余预算消耗过快或即将消耗完之前，SRE 有权中止和拒绝任何线上变更。





##### 落地SLO

先设定核心链路的 SLO，然后根据核心链路进行 SLO 的分解。

**第一步：整理核心链路：**确定核心应用与强弱依赖关系  ==> 根据pinpoint获取依赖关系【区分高可用 + 可降级】；核心应用之间的依赖关系，我们称之为强依赖，而其它应用之间的依赖关系，我们称之为弱依赖.



**第二步：设定 SLO , 准守以下原则**

* 核心应用的 SLO 要更严格，非核心应用可以放宽
* 强依赖之间的核心应用，SLO 要一致。例如交易和促销的
* 弱依赖中，核心应用对非核心的依赖，要有降级、熔断和限流等服务治理手段。
* Error Budget 策略，核心应用的错误预算要共享，就是如果某个核心应用错误预算消耗完，SLO 没有达成，那整条链路，原则上是要全部暂停操作的。



**第三步：验证核心链路的 SLO**

* 容量压测：容量目标是否可以达成。对于一般的业务系统，我们都会用 QPS 和 TPS 来表示系统容量，得到了容量这个指标，你就可以在平时观察应用或系统运行的容量水位情况。
* 混沌工程：做到在线上模拟真实的故障，做线上应急演练，提前发现隐患。







#### On-Call机制

##### 故障响应

MTTI：从发现故障到响应故障  ==> 从发现故障到影响故障的oncall机制

第一件事，判断出现的问题是不是故障；==> 监控告警 到 开发确认

第二件事，确定由谁来响应和召集 。On-Call 的流程机制建设：

* 确保关键角色在线。包含整个产品体系中的所有关键角色。
* 组织 War Room 应急组织。【建立消防群】
* 建立合理的呼叫方式；不是每次都找最熟悉业务的那个人【轮流值班】
* 确保资源投入的升级机制。搞不定，要求上级主管协调资源投入



##### 故障处理

在故障处理过程中采取的所有手段和行动，一切以恢复业务为最高优先级

关键角色分工：

* Incident Commander，故障指挥官，简称为 IC  ==> 组织和协调，而不是执行
* Communication Lead，沟通引导，简称 CL  ==> 责对内和对外的信息收集及通报
* Operations Lead，运维指挥，简称 OL  ==> 负责指挥或指导各种故障预案的执行和业务恢复。
* Incident Responders，简称 IR ==> 就是所有需要参与到故障处理中的各类人员



流程机制：

* 故障发现后，On-Call 的 SRE 或运维，最一开始就是 IC，有权召集相应的业务开发或其它必要资源，快速组织 War Room。
* 如果问题和恢复过程非常明确，IC 仍然是 SRE，就不做转移，由他来指挥每个人要做的具体事情，以优先恢复业务优先。
* 如果问题疑难，影响范围很大，这时 SRE 可以要求更高级别的主管介入，比如 SRE 主管或总监等，一般的原则是谁的业务受影响最大，谁来牵头组织。这时 SRE 要将 IC 的职责转移给更高级别的主管，如果是全站范围的影响，必要时技术 VP 或 CTO 也可以承担 IC 职责，或者授权给某位总监承担。







#### 互联网SRE组织架构



![image-20210608212544143](/Users/zhangxiaoxia/git_dir/learn/images/SRE互联网组织架构图.png)

SRE = PE【Production Engineer ==> 软件工程师】 + 工具平台开发 + 稳定性平台开发

DevOps 主要是以驱动价值交付为主，搭建企业内部的功效平台。SRE 主要需要协调多团队合作来提高稳定性。


